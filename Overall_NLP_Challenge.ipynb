{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "I will be using two Chesterton texts, Father Brown and The Man Who Was Thursday, along with two Austen texts, Emma and Persuasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "#Search Gutenberg Corpus and find texts\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Processing and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw text\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Inspect raw files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n",
      "\n",
      "\n",
      " END\n",
      "[Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\n",
      "for his own amusement, never took up any book but the Baronetage;\n",
      "there he found occupation for an idle hour, and consolation in a\n",
      "distressed one; there his faculties were roused into admiration and\n",
      "respect, by contemplating the limited remnant of the earliest patents;\n",
      "there any unwelcome sensations, arising from domestic affairs\n",
      "changed naturally into pity and contempt as he turn\n",
      "\n",
      "\n",
      " END\n",
      "[The Wisdom of Father Brown by G. K. Chesterton 1914]\n",
      "\n",
      "\n",
      "I. The Absence of Mr Glass\n",
      "\n",
      "\n",
      "THE consulting-rooms of Dr Orion Hood, the eminent criminologist\n",
      "and specialist in certain moral disorders, lay along the sea-front\n",
      "at Scarborough, in a series of very large and well-lighted french windows,\n",
      "which showed the North Sea like one endless outer wall of blue-green marble.\n",
      "In such a place the sea had something of the monotony of a blue-green dado:\n",
      "for the chambers themselves were ruled throughout by a \n",
      "\n",
      "\n",
      " END\n",
      "[The Man Who Was Thursday by G. K. Chesterton 1908]\n",
      "\n",
      "To Edmund Clerihew Bentley\n",
      "\n",
      "A cloud was on the mind of men, and wailing went the weather,\n",
      "Yea, a sick cloud upon the soul when we were boys together.\n",
      "Science announced nonentity and art admired decay;\n",
      "The world was old and ended: but you and I were gay;\n",
      "Round us in antic order their crippled vices came--\n",
      "Lust that had lost its laughter, fear that had lost its shame.\n",
      "Like the white lock of Whistler, that lit our aimless gloom,\n",
      "Men showed their \n"
     ]
    }
   ],
   "source": [
    "#Print first 500 characters of each, followed by 'END'\n",
    "print(emma[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(persuasion[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(brown[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(thursday[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clean and Process the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(r'Chapter \\d+', '', text)\n",
    "    text = re.sub(r'VOLUME \\w+', '', text)\n",
    "    text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text cleaning utility on our texts\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parse the Texts **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned texts\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8905\n",
      "3656\n",
      "3716\n",
      "3490\n"
     ]
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "\n",
    "print(len(emma_sents))\n",
    "print(len(persuasion_sents))\n",
    "print(len(brown_sents))\n",
    "print(len(thursday_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#For computational purposes, reduce length of each set of sentences to 500\n",
    "emma_sents = emma_sents[0:500]\n",
    "persuasion_sents = persuasion_sents[0:500]\n",
    "brown_sents = brown_sents[0:500]\n",
    "thursday_sents = thursday_sents[0:500]\n",
    "\n",
    "print(len(emma_sents))\n",
    "print(len(persuasion_sents))\n",
    "print(len(brown_sents))\n",
    "print(len(thursday_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4                                 (Between, _, them)  Austen"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Austen' 'Chesterton']\n"
     ]
    }
   ],
   "source": [
    "# Combine the sentences from the four texts into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + brown_sents + thursday_sents)\n",
    "\n",
    "#Confirm this worked\n",
    "display(sentences.head(5))\n",
    "\n",
    "#Confirm we only have two authors across four texts\n",
    "print(sentences.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 1000 most common words\n",
    "\n",
    "def bag_of_words(text):\n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "        \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Create column headers for sentence text and source (author) and initialize to 0\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags for each text\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + brownwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recover</th>\n",
       "      <th>love</th>\n",
       "      <th>could</th>\n",
       "      <th>surely</th>\n",
       "      <th>morbid</th>\n",
       "      <th>sure</th>\n",
       "      <th>mile</th>\n",
       "      <th>flower</th>\n",
       "      <th>speak</th>\n",
       "      <th>an</th>\n",
       "      <th>...</th>\n",
       "      <th>here</th>\n",
       "      <th>doctor</th>\n",
       "      <th>night</th>\n",
       "      <th>cigar</th>\n",
       "      <th>close</th>\n",
       "      <th>indulge</th>\n",
       "      <th>doubt</th>\n",
       "      <th>powerful</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(_)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(it, was, more, the, intimacy, of, sisters, .)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Even, before, Miss, Taylor, had, ceased, to, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, real, evils, ,, indeed, ,, of, Emma, 's,...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, danger, ,, however, ,, was, at, present,...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 2035 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  recover love could surely morbid sure mile flower speak an     ...     here  \\\n",
       "0       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "1       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "2       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "3       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "4       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "5       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "6       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "7       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "8       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "9       0    0     0      0      0    0    0      0     0  0     ...        0   \n",
       "\n",
       "  doctor night cigar close indulge doubt powerful  \\\n",
       "0      0     0     0     0       0     0        0   \n",
       "1      0     0     0     0       0     0        0   \n",
       "2      0     0     0     0       0     0        0   \n",
       "3      0     0     0     0       0     0        0   \n",
       "4      0     0     0     0       0     0        0   \n",
       "5      0     0     0     0       0     0        0   \n",
       "6      0     0     0     0       0     0        0   \n",
       "7      0     0     0     0       0     0        0   \n",
       "8      0     0     0     0       0     0        0   \n",
       "9      0     0     0     0       0     0        0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...      Austen  \n",
       "1  (She, was, the, youngest, of, the, two, daught...      Austen  \n",
       "2  (Her, mother, had, died, too, long, ago, for, ...      Austen  \n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...      Austen  \n",
       "4                                 (Between, _, them)      Austen  \n",
       "5                                                (_)      Austen  \n",
       "6     (it, was, more, the, intimacy, of, sisters, .)      Austen  \n",
       "7  (Even, before, Miss, Taylor, had, ceased, to, ...      Austen  \n",
       "8  (The, real, evils, ,, indeed, ,, of, Emma, 's,...      Austen  \n",
       "9  (The, danger, ,, however, ,, was, at, present,...      Austen  \n",
       "\n",
       "[10 rows x 2035 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models with BoW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chesterton    1000\n",
       "Austen        1000\n",
       "Name: text_source, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline of our dataset is 50%\n"
     ]
    }
   ],
   "source": [
    "#Determine Baseline\n",
    "display(word_counts.text_source.value_counts())\n",
    "print('\\n\\nBaseline of our dataset is 50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, establish X and Y\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regular Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.99\n",
      "\n",
      "Test set score: 0.776\n",
      "\n",
      "Cross-Validation: [0.705  0.6775 0.705  0.69   0.6825]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr = LogisticRegression(penalty='l2', C=1e9)\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lasso Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9253333333333333\n",
      "\n",
      "Test set score: 0.832\n",
      "\n",
      "Cross-Validation: [0.765  0.8225 0.8075 0.815  0.785 ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_lasso = LogisticRegression(penalty='l1')\n",
    "train = lr_lasso.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_lasso.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_lasso.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_lasso, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ridge Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.972\n",
      "\n",
      "Test set score: 0.858\n",
      "\n",
      "Cross-Validation: [0.81   0.8525 0.775  0.8425 0.8225]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_ridge = LogisticRegression(penalty='l2')\n",
    "train = lr_ridge.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_ridge.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_ridge.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_ridge, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Support Vector Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.5033333333333333\n",
      "\n",
      "Test set score: 0.494\n",
      "\n",
      "Cross-Validation: [0.505  0.5175 0.5275 0.555  0.5375]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "svc = SVC()\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(svc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forest Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9846666666666667\n",
      "\n",
      "Test set score: 0.816\n",
      "\n",
      "Cross-Validation: [0.695  0.765  0.7475 0.78   0.795 ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(rfc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#Create vectorizer model in order to get tf-idf for each sentence\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Take pandas series (text_sentence), convert from spacy object to string\n",
    "sentence_list = word_counts['text_sentence'].astype(str)\n",
    "print(type(sentence_list))\n",
    "\n",
    "#Pass pandas series to our vectorizer model\n",
    "text_tfidf = vectorizer.fit_transform(sentence_list)\n",
    "print(type(text_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Emma': 0.25307603703795595,\n",
       " 'Miss': 0.23365980549142268,\n",
       " 'Mr': 0.17835184836827853,\n",
       " 'Taylor': 0.28587845699241166,\n",
       " 'Woodhouse': 0.281913519641498,\n",
       " 'daughters': 0.3489390573977511,\n",
       " 'family': 0.264185193704954,\n",
       " 'fond': 0.3118541788615823,\n",
       " 'friend': 0.2838592675353782,\n",
       " 'governess': 0.38602393593391987,\n",
       " 'particularly': 0.3153071298140155,\n",
       " 'years': 0.26557493860358383}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#Shape\n",
    "n = text_tfidf.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*text_tfidf.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = text_tfidf[i, j]\n",
    "\n",
    "#Show first dictionary\n",
    "display(tfidf_bysent[3])\n",
    "print(type(tfidf_bysent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe for this feature set\n",
    "tfidf_df = pd.DataFrame(columns=terms)\n",
    "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
    "tfidf_df['text_source'] = word_counts['text_source']\n",
    "tfidf_df.loc[:, terms] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in tfidf_bysent:\n",
    "    for k, v in i.items():\n",
    "        tfidf_df.loc[counter, k] = v\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, establish X and Y\n",
    "\n",
    "Y = tfidf_df['text_source']\n",
    "X = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regular Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9926666666666667\n",
      "\n",
      "Test set score: 0.834\n",
      "\n",
      "Cross-Validation: [0.76   0.8225 0.79   0.78   0.7775]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr = LogisticRegression(penalty='l2', C=1e9)\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lasso Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8486666666666667\n",
      "\n",
      "Test set score: 0.804\n",
      "\n",
      "Cross-Validation: [0.71   0.79   0.8175 0.765  0.775 ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_lasso = LogisticRegression(penalty='l1')\n",
    "train = lr_lasso.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_lasso.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_lasso.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_lasso, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ridge Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.972\n",
      "\n",
      "Test set score: 0.866\n",
      "\n",
      "Cross-Validation: [0.7925 0.8725 0.8475 0.855  0.86  ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_ridge = LogisticRegression(penalty='l2')\n",
    "train = lr_ridge.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_ridge.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_ridge.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_ridge, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Support Vector Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.5026666666666667\n",
      "\n",
      "Test set score: 0.492\n",
      "\n",
      "Cross-Validation: [0.7525 0.8575 0.6725 0.7625 0.82  ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "svc = SVC()\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(svc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forest Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9893333333333333\n",
      "\n",
      "Test set score: 0.832\n",
      "\n",
      "Cross-Validation: [0.7275 0.8125 0.81   0.7975 0.82  ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(rfc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick one model and try to increase accuracy by 5% - BoW Feature Set, Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count of Parts of Speech Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for getting parts of speech\n",
    "\n",
    "def get_pos(df):\n",
    "    all_pos= []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        for token in sentence:\n",
    "            if token.pos != 0:\n",
    "                all_pos.append(token.pos_)\n",
    "    return all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unique column for each entity in words_count\n",
    "\n",
    "pos_types = set(get_pos(word_counts))\n",
    "pos = pd.DataFrame(columns=pos_types)\n",
    "word_counts = pd.concat([word_counts, pos], axis=1)\n",
    "word_counts.loc[:, pos_types] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, produce counts for each entity by sentence\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    for token in sentence:\n",
    "        if token.pos != 0:\n",
    "            word_counts.loc[i, token.pos_] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count of Entities Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for getting entity types\n",
    "\n",
    "def get_entity_types(df):\n",
    "    all_entities= []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        for token in sentence:\n",
    "            if token.ent_type != 0:\n",
    "                all_entities.append(token.ent_type_)\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unique column for each entity in words_count\n",
    "\n",
    "entity_types = set(get_entity_types(word_counts))\n",
    "entities = pd.DataFrame(columns=entity_types)\n",
    "word_counts = pd.concat([word_counts, entities], axis=1)\n",
    "word_counts.loc[:, entity_types] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, produce counts for each entity by sentence\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    for token in sentence:\n",
    "        if token.ent_type != 0:\n",
    "            word_counts.loc[i, token.ent_type_] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count of Words in Sentence Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of words in sentence\n",
    "\n",
    "word_counts['word_count'] = 0\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    words = len([token for token in sentence if not token.is_punct])\n",
    "        \n",
    "    # Populate the row with word counts.\n",
    "    word_counts.loc[i, 'word_count'] += words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pass the new feature set into the Lasso Logistic Regression Model again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, establish X and Y\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the right parameters\n",
    "\n",
    "def run_model(model):\n",
    "    if model == 'Lasso':\n",
    "        lasso = np.arange(0.2, 1.0, 0.2)\n",
    "        lasso_df = pd.DataFrame()\n",
    "        for i in lasso:\n",
    "            lr_lasso = LogisticRegression(penalty='l1', C=i)\n",
    "            lr_lasso.fit(X, Y)\n",
    "            positive_pred = lr_lasso.predict(X)\n",
    "            cross_val = cross_val_score(lr_lasso, X, Y, cv=5).mean()*100\n",
    "            score = lr_lasso.score(X, Y)*100\n",
    "            lasso_df_temp = pd.DataFrame({'C': i, 'training': score, 'test': cross_val}, index=[0])\n",
    "            lasso_df = lasso_df.append(lasso_df_temp)\n",
    "        plt.plot(lasso_df['C'], lasso_df['training'], color = 'r', linewidth = 3, label='Training')\n",
    "        plt.plot(lasso_df['C'], lasso_df['test'], color = 'b', linewidth = 3, label='Test')\n",
    "        plt.xlabel('C Parameter', fontsize=20)\n",
    "        plt.title('Lasso - Training vs. Test')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1178be4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9186666666666666\n",
      "\n",
      "Test set score: 0.842\n",
      "\n",
      "Cross-Validation: [0.7575 0.7975 0.7975 0.8025 0.7925]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_lasso = LogisticRegression(penalty='l1')\n",
    "train = lr_lasso.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_lasso.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_lasso.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_lasso, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After adding new features, I saw a substantive increase in accuracy in the lasso logistic regression model.\n",
    "\n",
    "**Original:** Training 84.86%; Test 80.40%\n",
    "**Updated:** Training 91.86%; Test 84.20%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
